{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "important-repair",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "import re\n",
    "from submit_cutadapt import submit_cutadapt\n",
    "from get_consensus_seq_from_cdhit import get_consensus_seq_from_cdhit\n",
    "from remove_chimeras_uchime import remove_chimeras_uchime\n",
    "from mkdir import mkdir\n",
    "\n",
    "def merge_contigs(contig_list, merge_fa_name):\n",
    "    '''合并组装后contigs\n",
    "    参数：\n",
    "        contig_list: contig文件路径列表（type=list）\n",
    "        merge_fa_name: 输出文件\n",
    "    返回：\n",
    "        df: 每个umiID的contig数目（type=pandas）\n",
    "    '''\n",
    "    seq_no_dict = {}\n",
    "    with open(merge_fa_name, 'w') as w:\n",
    "        m = 0\n",
    "        for record in contig_list:\n",
    "            name = record[0]\n",
    "            fa = record[1]\n",
    "            tmp = []\n",
    "            n = 0\n",
    "            for seq in SeqIO.parse(fa, 'fasta'):\n",
    "                seq.description = str(name) + \"|\" + str(seq.description)\n",
    "                seq.id = str(m) + '_' + str(n)\n",
    "                tmp.append(seq)\n",
    "                n += 1\n",
    "            seq_no = SeqIO.write(tmp, w, 'fasta')\n",
    "            seq_no_dict[name] = [m, seq_no]\n",
    "            m += 1\n",
    "    return pd.DataFrame.from_dict(seq_no_dict, orient='index',\n",
    "                                  columns=(['umi_id', 'NO_of_Contigs']))\n",
    "\n",
    "def cut_fa_by_len(input_fa, output_fa, min_len=0, max_len=0):\n",
    "    '''筛选fasta文件符合长度要求的序列\n",
    "    参数：\n",
    "        input_fa: 输入fasta文件\n",
    "        output_fa: 输出fasta文件\n",
    "        min_len: 序列最小长度\n",
    "        max_len: 序列最大长度\n",
    "    返回：\n",
    "        c: 过滤后序列数量\n",
    "    '''\n",
    "    tmp = []\n",
    "    if max_len == 0:\n",
    "        for seq in SeqIO.parse(input_fa, 'fasta'):\n",
    "            l = len(seq.seq)\n",
    "            if l >= min_len:\n",
    "                seq.description = 'len=' + str(l)\n",
    "                tmp.append(seq)\n",
    "    else:\n",
    "        for seq in SeqIO.parse(input_fa, 'fasta'):\n",
    "            l = len(seq.seq)\n",
    "            if l >= min_len and l < max_len:\n",
    "                seq.description = 'len=' + str(l)\n",
    "                tmp.append(seq)\n",
    "    c = SeqIO.write(tmp, output_fa, 'fasta')\n",
    "    return c\n",
    "\n",
    "def submit_cdhit(infa, outfa, identity, path, threads=1):\n",
    "    '''序列聚类\n",
    "    参数：\n",
    "        infa: 输入fasta序列\n",
    "        outfa: 输出fasta代表序列\n",
    "        identity: 相似度\n",
    "        path: cd-hit路径\n",
    "        threads: 线程数\n",
    "    返回：\n",
    "        无\n",
    "    '''\n",
    "    cdhit = path\n",
    "    str_join = ' '\n",
    "    cmd = str_join.join([cdhit,\n",
    "                         '-i', infa,\n",
    "                         '-o', outfa,\n",
    "                         '-c', str(identity),\n",
    "                         '-T', str(threads),\n",
    "                         '-M', str(0)]\n",
    "                        )\n",
    "    logging.info(' %s' % cmd)\n",
    "    status = os.system(cmd)\n",
    "    if status == 0:\n",
    "        logging.info(' done')\n",
    "    else:\n",
    "        logging.info(' exit')\n",
    "        \n",
    "def update_seqID(infa, outfa, tag):\n",
    "    '''重新命名序列\n",
    "    参数：\n",
    "        infa: 输入fasta文件\n",
    "        outfa: 输出fasta文件\n",
    "        tag: 序列标签\n",
    "    返回：\n",
    "        id_track: 序列ID对应关系（type=dict）\n",
    "    '''\n",
    "    n = 0\n",
    "    id_track = {}\n",
    "    seqs = []\n",
    "    for rec in SeqIO.parse(infa, 'fasta'):\n",
    "        new_id = tag + '_' + str(n)\n",
    "        id_track[new_id] = rec.id\n",
    "        rec.id = new_id\n",
    "        rec.name = new_id\n",
    "        rec.description = str(rec.description).split()[-1]\n",
    "        n += 1\n",
    "        seqs.append(rec)\n",
    "    count = SeqIO.write(seqs, outfa, 'fasta')\n",
    "    return id_track\n",
    "\n",
    "def get_final_contigs_from_id(rawfa, idlist, outfile, prefix='contig'):\n",
    "    raw=[]\n",
    "    if os.path.splitext(rawfa)[1] in ['.fa','.fasta','.FASTA', '.Fasta', '.FA']:\n",
    "        raw = list(SeqIO.parse(rawfa, 'fasta'))\n",
    "    else:\n",
    "        try:\n",
    "            with gzip.open(rawfa, 'rt') as h:\n",
    "                raw = list(SeqIO.parse(h, 'fasta'))\n",
    "        except Exception as e:\n",
    "            print('error: {}'.format(e))\n",
    "    contigs = [i for i in raw if i.id in idlist]\n",
    "    for i in contigs:\n",
    "        i.id = str(prefix) + '_' + str(i.id)\n",
    "        i.description = ''\n",
    "    count = SeqIO.write(contigs, outfile, 'fasta')\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "alien-bosnia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterContigs(contigs_dir, output_dir, \n",
    "                  File_Tag='test',\n",
    "                  minlength=1200, maxlength=1700, \n",
    "                  cdhit='cd-hit',\n",
    "                  cutadapt='cutadapt',\n",
    "                  usearch='usearch11',\n",
    "                  file_primer_rc='adapter_fa/Lrc_1-8.fa',\n",
    "                  file_primer='adapter_fa/L_1-8.fa',\n",
    "                  threads=2,\n",
    "                 ):\n",
    "    spades_path = contigs_dir\n",
    "    mkdir(output_dir)\n",
    "    result_dir = output_dir\n",
    "    ana_dir = output_dir\n",
    "    merge_fa = os.path.join(result_dir, File_Tag+'.merged.fasta')\n",
    "    merge_trim_fa1 = os.path.join(ana_dir, File_Tag+'.merged.trim1.fasta')\n",
    "    merge_trim_log1 = os.path.join(ana_dir, File_Tag+'.merged.trim1.log')\n",
    "    merge_trim_fa2 = os.path.join(result_dir, File_Tag+'.merged.trim.fasta')\n",
    "    merge_trim_log2 = os.path.join(ana_dir, File_Tag+'.merged.trim2.log')\n",
    "    merge_filter_fa = os.path.join(ana_dir, File_Tag+'.merged.filter.fasta')\n",
    "    merge_filter_cluster_fa = os.path.join(ana_dir, File_Tag+'.merged.filter.rep.fa')\n",
    "    clust_fa = os.path.join(ana_dir, File_Tag+'.clust.fasta')\n",
    "    clust_fa_clstr_table = os.path.join(result_dir, File_Tag+'.clust.clstr.info')\n",
    "    clust_uchimeout = os.path.join(ana_dir, File_Tag+'.clust.uchimeout.out.txt')\n",
    "    clust_ch_fa = os.path.join(ana_dir, File_Tag+'.clust.ch.fasta')\n",
    "    clust_nonch_fa = os.path.join(ana_dir, File_Tag+'.clust.nonch.fasta')\n",
    "    final_fa = os.path.join(result_dir, File_Tag+'.final.fasta')\n",
    "    final_tab = os.path.join(result_dir, File_Tag+'.final.size.tab.txt')\n",
    "    ID_info = os.path.join(result_dir, File_Tag+'.final.id.info')\n",
    "    ID_contig_info = os.path.join(result_dir, File_Tag+'.final.contig.id.info')\n",
    "    final_contig_fa = os.path.join(result_dir, File_Tag+'.final.contig.fasta')\n",
    "\n",
    "    #合并组装后的contigs\n",
    "    assemble_list = []\n",
    "    for tmpdir in os.listdir(spades_path):\n",
    "        for d in os.listdir(os.path.join(spades_path, tmpdir)):\n",
    "            contig_file = os.path.join(spades_path, tmpdir, d, 'contigs.fasta')\n",
    "            if os.path.exists(contig_file):\n",
    "                tmp = [d,contig_file]\n",
    "                assemble_list.append(tmp)\n",
    "    df_merge_fa = merge_contigs(assemble_list,merge_fa)\n",
    "    \n",
    "    #去除contig中引物序列\n",
    "    logging.info('remove primer in contigs')\n",
    "    submit_cutadapt(merge_fa, \n",
    "                    merge_trim_fa1, \n",
    "                    merge_trim_log1,\n",
    "                    \"file:\"+file_primer_rc,\n",
    "                    'a',\n",
    "                    cutadapt,\n",
    "                    threads=threads\n",
    "                   )\n",
    "    submit_cutadapt(merge_trim_fa1,\n",
    "                    merge_trim_fa2,\n",
    "                    merge_trim_log2,\n",
    "                    \"file:\"+file_primer,\n",
    "                    'g',\n",
    "                    cutadapt,\n",
    "                    threads=threads\n",
    "                   )\n",
    "    #长度过滤 大于minlength小于maxlength\n",
    "    tmp = cut_fa_by_len(merge_trim_fa2,\n",
    "                        merge_filter_fa,\n",
    "                        minlength,\n",
    "                        maxlength)\n",
    "    logging.info('Contigs after length filtered({}-{}):\\t{}'.format(minlength, maxlength, tmp))\n",
    "    \n",
    "    #聚类 100%相似度 cd-hit\n",
    "    logging.info('contig cluster start')\n",
    "    submit_cdhit(merge_filter_fa, \n",
    "                 merge_filter_cluster_fa, \n",
    "                 1, \n",
    "                 cdhit, \n",
    "                 threads)\n",
    "    \n",
    "    #去除同一umiID中未聚类在同一组的contig\n",
    "    logging.info('get consensus seq')\n",
    "    consensus_seq_count, rep_seq_tab = get_consensus_seq_from_cdhit(\n",
    "        merge_filter_cluster_fa+'.clstr', \n",
    "        merge_filter_fa, \n",
    "        clust_fa)\n",
    "    \n",
    "    pd.DataFrame(rep_seq_tab,\n",
    "                 columns=(['clust_rep_id','size','seq_id'])).to_csv(clust_fa_clstr_table, \n",
    "                                                                  sep='\\t',\n",
    "                                                                  index=False)\n",
    "    #去除嵌合\n",
    "    logging.info('remove cluster chimeras start')\n",
    "    remove_chimeras_uchime(clust_fa,\n",
    "                           clust_uchimeout,\n",
    "                           clust_ch_fa,\n",
    "                           clust_nonch_fa,\n",
    "                           usearch)\n",
    "    dict_update_id = update_seqID(clust_nonch_fa, final_fa, File_Tag)\n",
    "    \n",
    "    # 生成cluster size表格\n",
    "    tab_list = []\n",
    "    total_size = 0\n",
    "    total_cluster = 0\n",
    "    for i in SeqIO.parse(final_fa, 'fasta'):\n",
    "        size = re.search('size=(\\d+)', i.description).group(1)\n",
    "        tab_list.append([i.id, size])\n",
    "        total_size += int(size)\n",
    "        total_cluster += 1\n",
    "    pd.DataFrame(tab_list, columns=(['id', File_Tag])).set_index('id').to_csv(final_tab, sep='\\t')\n",
    "    logging.info('Cluster after remove Chimerias:\\t{}'.format(total_cluster))\n",
    "    logging.info('Contigs after remove Chimerias:\\t{}'.format(total_size))\n",
    "    \n",
    "    #合并并输出代表序列对应的ID信息\n",
    "    df_merge_fa = df_merge_fa.reset_index().rename(columns={'index': 'umiID'})\n",
    "    df_update_id = pd.DataFrame.from_dict(dict_update_id,\n",
    "                                          orient='index',\n",
    "                                          columns=(['contig_id']))\n",
    "    df_update_id['umi_id'] = pd.to_numeric(df_update_id['contig_id'].str.split('_').str[0])\n",
    "    df_ID_info = pd.merge(df_update_id.reset_index().rename(columns={'index': 'ID'}),\n",
    "                          df_merge_fa,\n",
    "                          left_on='umi_id',\n",
    "                          right_on='umi_id',\n",
    "                          how='left')\n",
    "    df_ID_info.to_csv(ID_info, sep='\\t')\n",
    "    #提取并输出所有cluster的contigs对应的umiID信息\n",
    "    df_tmp = df_merge_fa[['umi_id','umiID']].set_index('umi_id')\n",
    "    # df_tmp.to_csv(os.path.join(result_dir, 'df_merge_fa.csv'))\n",
    "    total_id = df_tmp.to_dict()['umiID']\n",
    "    final_clust = df_update_id['contig_id'].to_list()\n",
    "    final_total_contig = {}\n",
    "    for c in rep_seq_tab:\n",
    "        if c[0] in final_clust:\n",
    "            for i in c[2].split('|'):\n",
    "                umiid = int(i.split('_')[0])\n",
    "                final_total_contig[i] = total_id[umiid]\n",
    "    c = get_final_contigs_from_id(merge_trim_fa2,\n",
    "                                  final_total_contig.keys(),\n",
    "                                  final_contig_fa,\n",
    "                                  prefix=File_Tag)\n",
    "    return final_tab, final_fa, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "reliable-heater",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-28 17:01:33,583 INFO: remove primer in contigs\n",
      "2021-02-28 17:01:33,584 INFO: ../venv/bin/cutadapt -a file:../adapter_fa/Lrc_1-8.fa -o ../test/out_dir_2/test/F1_R3/F1_R3.merged.trim1.fasta ../test/out_dir_2/test/F1_R3/F1_R3.merged.fasta -e 2 -j 1 > ../test/out_dir_2/test/F1_R3/F1_R3.merged.trim1.log\n",
      "2021-02-28 17:01:33,911 INFO: ../venv/bin/cutadapt -g file:../adapter_fa/L_1-8.fa -o ../test/out_dir_2/test/F1_R3/F1_R3.merged.trim.fasta ../test/out_dir_2/test/F1_R3/F1_R3.merged.trim1.fasta -e 2 -j 1 > ../test/out_dir_2/test/F1_R3/F1_R3.merged.trim2.log\n",
      "2021-02-28 17:01:34,072 INFO: No. of Contigs after length filtered(1200-1700):\t14\n",
      "2021-02-28 17:01:34,073 INFO: contig cluster start\n",
      "2021-02-28 17:01:34,073 INFO: ../venv/bin/cd-hit -i ../test/out_dir_2/test/F1_R3/F1_R3.merged.filter.fasta -o ../test/out_dir_2/test/F1_R3/F1_R3.merged.filter.rep.fa -c 1 -T 2 -M 0\n",
      "2021-02-28 17:01:34,166 INFO: done\n",
      "2021-02-28 17:01:34,167 INFO: get consensus seq\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'../test/out_dir_2/test/F1_R3/F1_R3.merged.filter.fasta'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s',level='INFO')\n",
    "\n",
    "# contigs_dir = '../test/out_dir_2/test/F1_R3_spades'\n",
    "# output_dir = '../test/out_dir_2/test/F1_R3/'\n",
    "# File_Tag = 'F1_R3'\n",
    "# filterContigs(contigs_dir, output_dir, \n",
    "#               File_Tag=File_Tag, minlength=1200, maxlength=1700, \n",
    "#               cdhit='../venv/bin/cd-hit',\n",
    "#               cutadapt='../venv/bin/cutadapt',\n",
    "#               file_primer_rc='../adapter_fa/Lrc_1-8.fa',\n",
    "#               file_primer='../adapter_fa/L_1-8.fa',\n",
    "#               threads=2,\n",
    "#              )"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
