{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "favorite-shelf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from pathos.multiprocessing import ProcessingPool as Pool\n",
    "from mkdir import mkdir\n",
    "from Bio import SeqIO\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "convertible-chorus",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkSeqIDFile(seqID, df_Aread2uID, out_dir='./'):\n",
    "    seqIDfile = os.path.join(out_dir, seqID+\".id\")\n",
    "    df_Aread2uID[df_Aread2uID['umiID']==seqID]['AreadID'].to_csv(seqIDfile, index=False, header=False)\n",
    "    return seqIDfile\n",
    "\n",
    "\n",
    "def getSeqByID(seqID, rawdata, df_Aread2uID, out_dir='./', seqkit_path='../venv/bin/seqkit'):\n",
    "    mkdir(out_dir)\n",
    "    seqIDfile = mkSeqIDFile(seqID, df_Aread2uID, out_dir)\n",
    "    cmd = \"{seqkit} grep --pattern-file {seqIDfile} {rawdata} >{seqID}.fq\".format(seqkit=seqkit_path, \n",
    "                                                                                  seqIDfile=seqIDfile, \n",
    "                                                                                  rawdata=rawdata,\n",
    "                                                                                  seqID=os.path.join(out_dir, seqID),\n",
    "                                                                                 )\n",
    "    try:\n",
    "        os.system(cmd)\n",
    "        os.remove(seqIDfile)\n",
    "        seq_count = len(list(SeqIO.parse(os.path.join(out_dir, seqID+'.fq'), 'fastq')))\n",
    "        return seq_count\n",
    "    except Exception as e:\n",
    "        logging.info(e)\n",
    "        return 0\n",
    "    \n",
    "    \n",
    "    \n",
    "def getSeqById_multi(seqID_list, rawdata, df_Aread2uID, out_dir='./', seqkit_path='../venv/bin/seqkit', \n",
    "                     func=getSeqByID, dir_files=1000):\n",
    "    def pool_sub(func, seqID_list_tmp, rawdata_list, df_Aread2uID_list, out_dir_tmp_list, seqkit_path_list):\n",
    "        seq_count = 0\n",
    "        pool_write = Pool()\n",
    "        result = pool_write.map(func, seqID_list_tmp, rawdata_list, df_Aread2uID_list, out_dir_tmp_list, seqkit_path_list)\n",
    "        pool_write.close()\n",
    "        pool_write.join()\n",
    "        pool_write.clear()\n",
    "        for get in result:\n",
    "            seq_count += get\n",
    "        return seq_count\n",
    "    \n",
    "    total_seq_count = 0\n",
    "    n = len(seqID_list)\n",
    "    range_list = range(int(len(seqID_list)/dir_files))\n",
    "    for i, v in enumerate(range_list):\n",
    "        out_dir_tmp = os.path.join(out_dir, 'tmp_'+str(v))\n",
    "        if i==len(range_list)-1:\n",
    "            seqID_list_tmp = seqID_list[v*dir_files:]\n",
    "        else:\n",
    "            seqID_list_tmp = seqID_list[v*dir_files:range_list[i+1]*dir_files]\n",
    "        total_seq_count += pool_sub(func, seqID_list_tmp, [rawdata]*n, [df_Aread2uID]*n, [out_dir_tmp]*n, [seqkit_path]*n)\n",
    "    return total_seq_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "incorporate-calcium",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(file_name, seq_list):\n",
    "    count = 0\n",
    "#     file_name = tmp_list[0]\n",
    "#     file_name_seq_dict = tmp_list[1]\n",
    "    with open(file_name, 'a') as handle:\n",
    "        count += SeqIO.write(seq_list, handle, 'fastq')\n",
    "    return count\n",
    "\n",
    "def queu_group_umi_seq(seq, out_dir, aUMI_dict):\n",
    "    '''基于umi将序列分组（多进程）\n",
    "    参数：\n",
    "        seq: 输入序列文件（fastq）\n",
    "        out_dir: 分组序列输出目录\n",
    "        aUMI_dict: 序列与uID的对应关系（type=dict)\n",
    "        threads: 进程数\n",
    "        paired: 默认参数\n",
    "    返回：\n",
    "        umi_counts_sub: 每个umi的序列数（type=dict）\n",
    "        umi_id_counts_sub: 每个umi ID的序列数（type=dict）\n",
    "        umi2seq_tmp: 分组过程检测的序列数据（type=list）\n",
    "    '''\n",
    "#     func = 'group_into_umis'\n",
    "#     if paired == 0:\n",
    "#         func = 'group_into_umis_unpaired'\n",
    "\n",
    "    umi_counts_sub = {}\n",
    "    umi_id_counts_sub = {}\n",
    "    umi_file_list_sub = {}\n",
    "    umi_seq_dir_tmp_sub = {}\n",
    "    total_check_seq = 0\n",
    "\n",
    "    file_open = 'open'\n",
    "    if os.path.splitext(seq)[1] == '.gz':\n",
    "        file_open = 'gzip.open'\n",
    "\n",
    "    with eval(file_open)(seq, 'rt') as seq_handle:\n",
    "        seqs = SeqIO.parse(seq_handle, 'fastq')\n",
    "        total_umi_seq_dict = {}\n",
    "        queue = []\n",
    "        check_umiID = {}\n",
    "        count = 0\n",
    "        count_seq = 0\n",
    "        umi2seq_tmp = []\n",
    "        for seq_tmp in seqs:\n",
    "            if seq_tmp.id in aUMI_dict.keys():\n",
    "                umiID = aUMI_dict[seq_tmp.id]\n",
    "                count_seq += 1\n",
    "                total_umi_seq_dict[umiID] = total_umi_seq_dict.get(umiID, [])\n",
    "                total_umi_seq_dict[umiID].append(seq_tmp)\n",
    "                total_check_seq += 1\n",
    "                if (len(total_umi_seq_dict.keys()) > 5000 or total_check_seq >= 500000):\n",
    "                    file_name_seq_dict = {}\n",
    "                    queue_write_files = []\n",
    "                    queue_write_seqs = []\n",
    "                    for uid, record in total_umi_seq_dict.items():\n",
    "#                         umiID = umi2ID_dict[u]\n",
    "                        check_umiID[uid] = check_umiID.get(uid, 0) + len(record)\n",
    "                        dir_tmp = 'tmp_' + str(int(len(check_umiID.keys()) / 1000))\n",
    "                        dir_tmp_path = os.path.join(out_dir, dir_tmp)\n",
    "                        mkdir(dir_tmp_path)\n",
    "                        umi_seq_dir_tmp_sub[uid] = umi_seq_dir_tmp_sub.get(uid, dir_tmp)\n",
    "                        file_name = os.path.join(out_dir, umi_seq_dir_tmp_sub[uid], uid + '.fastq')\n",
    "                        queue_write_files.append(file_name)\n",
    "                        queue_write_seqs.append(record)\n",
    "#                         file_name_seq_dict[file_name] = file_name_seq_dict.get(file_name, {})\n",
    "#                         file_name_seq_dict[file_name] = record\n",
    "#                     for file_name in file_name_seq_dict.keys():\n",
    "#                         queue_write.append([file_name, file_name_seq_dict[file_name]])\n",
    "                    total_umi_seq_dict = {}\n",
    "#                     file_name_seq_dict = {}\n",
    "                    total_check_seq = 0\n",
    "#                     pool_write = multiprocessing.Pool(processes=threads)\n",
    "#                     result_write = pool_write.map(write_to_file, queue_write_files, queue_write_seqs)\n",
    "#                     pool_write.close()\n",
    "#                     pool_write.join()\n",
    "                    pool_write = Pool()\n",
    "                    result = pool_write.map(write_to_file, queue_write_files, queue_write_seqs)\n",
    "                    pool_write.close()\n",
    "                    pool_write.join()\n",
    "                    pool_write.clear()\n",
    "                    for get in result:\n",
    "                        count += get\n",
    "                    logging.info(' write {} / {} seqs of {} umi_IDs to files'. \\\n",
    "                                 format(count, count_seq, len(check_umiID.keys())))\n",
    "                    umi2seq_tmp.append([count, count_seq, len(check_umiID.keys())])\n",
    "    if total_umi_seq_dict:\n",
    "        file_name_seq_dict = {}\n",
    "        queue_write_files = []\n",
    "        queue_write_seqs = []\n",
    "        for uid, record in total_umi_seq_dict.items():\n",
    "            check_umiID[uid] = check_umiID.get(uid, 0) + len(record)\n",
    "            dir_tmp = 'tmp_' + str(int(len(check_umiID.keys()) / 1000))\n",
    "            dir_tmp_path = os.path.join(out_dir, dir_tmp)\n",
    "            mkdir(dir_tmp_path)\n",
    "            umi_seq_dir_tmp_sub[uid] = umi_seq_dir_tmp_sub.get(uid, dir_tmp)\n",
    "            file_name = os.path.join(out_dir, umi_seq_dir_tmp_sub[uid], uid + '.fastq')\n",
    "            queue_write_files.append(file_name)\n",
    "            queue_write_seqs.append(record)\n",
    "        total_umi_seq_dict = {}\n",
    "        total_check_seq = 0\n",
    "        pool_write = Pool()\n",
    "        result = pool_write.map(write_to_file, queue_write_files, queue_write_seqs)\n",
    "        pool_write.close()\n",
    "        pool_write.join()\n",
    "        pool_write.clear()\n",
    "        for get in result:\n",
    "            count += get\n",
    "        logging.info(' write {} / {} seqs of {} umi_IDs to files'. \\\n",
    "                     format(count, count_seq, len(check_umiID.keys())))\n",
    "        umi2seq_tmp.append([count, count_seq, len(check_umiID.keys())])\n",
    "\n",
    "    return umi2seq_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "waiting-howard",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s',level='INFO')\n",
    "\n",
    "# out_dir = '../test/out_dir_2/test/F1_R3'\n",
    "# uIDseq_dir = '../test/out_dir_2/test/F1_R3'\n",
    "# File_Tag = 'F1_R3'\n",
    "# seqkit_path = '../venv/bin/seqkit'\n",
    "# # A1_file = '../test/mnt/data/16S_FAST_data/210125_A00262_0590_AHVYVTDSXY/1-P_L2_Y0000085Y0000712.R1.fastq.gz'\n",
    "# A1_file = '../test/rawdata/1-P_R1_test.fastq.gz'\n",
    "\n",
    "# A_uID_file = os.path.join(out_dir, File_Tag+'_A_reads2uID.csv')\n",
    "# df_Aread2uID = pd.read_csv(A_uID_file, index_col=0)[['AreadID', 'umiID']]\n",
    "\n",
    "# uID_list = df_Aread2uID.head(1000)['umiID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "quarterly-better",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# aUMI_dict = df_Aread2uID[df_Aread2uID['umiID'].isin(uID_list)].set_index('AreadID').to_dict()['umiID']\n",
    "# Seq2uID_sta = queu_group_umi_seq(A1_file, uIDseq_dir, aUMI_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lovely-incident",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-01 14:33:54,949 INFO:uID count:\t10\n",
      "Process ForkPoolWorker-15:\n",
      "Process ForkPoolWorker-16:\n",
      "Process ForkPoolWorker-10:\n",
      "Process ForkPoolWorker-14:\n",
      "Process ForkPoolWorker-11:\n",
      "Process ForkPoolWorker-13:\n",
      "Traceback (most recent call last):\n"
     ]
    }
   ],
   "source": [
    "# logging.info(\"uID count:\\t{}\".format(len(uID_list)))\n",
    "# total_seq_count = getSeqById_multi(uID_list, A1_file, df_Aread2uID, uIDseq_dir, seqkit_path, dir_files=10)\n",
    "# logging.info(\"total seqs write to uID fq file:\\t{}\".format(total_seq_count))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
